---
title: "In-class Exercise 7: Global and Local Spatial Autocorrelation "
execute: 
  message: true
  warning: false
  echo: true
  eval: true
date: "20 February 2023"
date-modified: "`r Sys.Date()`"
# number-sections: true
editor: visual
format: html
---

# 1 Overview

This in-class introduces an alternative R package to spdep package you used in [Chapter 9: Global Measures of Spatial Autocorrelation](https://r4gdsa.netlify.app/chap09.html) and [Chapter 10: Local Measures of Spatial Autocorrelation](https://r4gdsa.netlify.app/chap10.html). The package is called [**sfdep**](https://sfdep.josiahparry.com/). According to Josiah Parry, the developer of the package, \"sfdep builds on the great shoulders of **spdep** package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.\"

# 2 Getting Started

## 2.1 Installing and Loading the R packages

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse)
```

# 3 The Data

## 3.1 Importing geospatial data

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

## 3.2 Importing attribute table

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

## 3.3 Combining both data frame by using a left join

```{r}
hunan_GDPPC <- left_join(hunan, hunan2012, by="County") |>
  select(1:4, 7, 15)
```

## 3.4 Plotting a choropleth map

```{r}
tmap_mode("plot")
tm_shape(hunan_GDPPC)+
  tm_fill("GDPPC", 
          style = "quantile", 
          palette = "Blues",
          title = "GDPPC") +
  tm_layout(main.title = "Distribution of GDP per capita by district, Hunan Province",
            main.title.position = "center",
            main.title.size = 1.0,
            legend.height = 0.40, 
            legend.width = 0.30,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

# 4 Global Measures of Spatial Association

## 4.1 Deriving contiguity weights: Queen's method

```{r}
wm_q <- hunan_GDPPC |>
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb, style='W'),
         .before = 1)
```
-   nb: A neighbor list object as created by st_neighbors().
-   style: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).
-   allow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.


## 4.2 Computing Global Moran's I

```{r}
moranI <- global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

## 4.3 Performing Global Moran's I Test

```{r}
global_moran_test(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

-   we can reject null hypothesis that the observed pattern is spatial independent, meaning that it is actually dependent
-   Moran I stats \> 0 meaning clustered, observations tend to be similar
-   this uses raw data

## 4.4 Performing Global Moran's I Permutation Test

```{r}
set.seed(1234)
```

-   set the seed value (just at the v beginning of permutations/simulations) so that the results will always be reproducible and constant

```{r}
global_moran_perm(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt,
                  nsim=99)
```

-   this is simulations not raw data
-   Moran's I test statistic is quite similar to section 4.3 but p-value here is more significant
-   simulations always give 2 sided test

# 5 Computing local Moran's I

```{r}
lisa <- wm_q |>
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim =99), 
    .before = 1) |>
  unnest(local_moran)
lisa
```

-   unnest() is needed
-   in the lisa df output:
    -   ii = local moran stats

    -   eii = standard error deviation

    -   var_ii = variance

    -   z = standard value

    -   p = p value

    -   p_ii_sim = p value based on simulations

    -   p_folded_sim = p value based on k fold methods and simulations

    -   mean = provides the labels of the clusters using R

    -   pysal = labels of the clusters using a python script

    -   median = labels of the clusters with non-parametric version

    -   in general mean & pysal should be the same (use either, can just use mean)

## 5.1 Visualising local Moran's I

```{r}
tmap_mode("plot")
tm_shape(lisa) + 
  tm_fill("ii") + 
  tm_borders(alpha=0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

-   

## 5.2 Visualising p-value of local Moran's I

```{r}
tmap_mode("plot")
tm_shape(lisa) + 
  tm_fill("p_ii") + 
  tm_borders(alpha=0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

```{r}
tmap_mode("plot")
tm_shape(lisa) + 
  tm_fill("p_ii_sim") + 
  tm_borders(alpha=0.5)
```

-   should actually use p_ii_sim instead of p_ii
-   p_ii is only based on the raw data u use
-   p_ii_sim / p_folded_sim is based on simulations (just use p_ii_sim)

## 5.3 Visualising local Moran's I and p-value
```{r}
tmap_mode("plot")
map1 <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```


# 6 Visualising local Moran's I

```{r}
lisa_sig <- lisa |>
  filter(p_ii < 0.05)
tmap_mode("plot")
tm_shape(lisa) + 
  tm_polygons() + 
  tm_borders(alpha=0.5) +
tm_shape(lisa_sig) + 
  tm_fill("mean") +
  tm_borders(alpha=0.4)
```

-   modify the code to include a class called "not significant" (refer to section 10.7.4

## 6.1 Deriving contiguity weights: Rooks method

```{r}
wm_r <- hunan %>%
  mutate(nb = st_contiguity(geometry,
                            queen = FALSE),
         wt = st_weights(nb),
         .before = 1) 
```

# 7 Distance-based Weights
There are three popularly used distance-based spatial weights, they are:

-   fixed distance weights,
-   adaptive distance weights, and
-   inverse distance weights (IDW).

## 7.1 Deriving Fixed Distance Weights
Before we can derive the fixed distance weights, we need to determine the upper limit for distance band by using the steps below:
```{r}
geo <- sf::st_geometry(hunan_GDPPC)
nb <- st_knn(geo, longlat = TRUE)
dists <- unlist(st_nb_dists(geo, nb))
```
-   st_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.
-   unlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived.

Now, we will go ahead to derive summary statistics of the nearest neighbour distances vector (i.e. dists) by usign the coced chunk below.
```{r}
summary(dists)
```

The summary statistics report above shows that the maximum nearest neighbour distance is 65.80km. By using a threshold value of 66km will ensure that each area will have at least one neighbour.

Now we will go ahead to compute the fixed distance weights by using the code chunk below.

```{r}
wm_fd <- hunan_GDPPC %>%
  dplyr::mutate(nb = st_dist_band(geometry,
                                  upper = 66),
               wt = st_weights(nb),
               .before = 1)
```
-   st_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).
-   st_weights() is then used to calculate polygon spatial weights of the nb list. Note that:
-   the default style argument is set to “W” for row standardized weights, and
-   the default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.

## 7.2 Deriving Adaptive Distance Weights
```{r}
wm_ad <- hunan_GDPPC %>% 
  mutate(nb = st_knn(geometry,
                     k=8),
         wt = st_weights(nb),
               .before = 1)
```

## 7.3 Calculate Inverse Distance Weights
```{r}
wm_idw <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wts = st_inverse_distance(nb, geometry,
                                   scale = 1,
                                   alpha = 1),
         .before = 1)
```


# HCSA - Hot Spot and Cold Spot Area Analysis

## Computing local Moran's I
```{r}
HCSA <- wm_q |>
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim=99),
      .before=1) |>
  unnest(local_Gi)
HCSA
```

-   in general, we will use g star not g
-   g star perm means run simulations, g star just runs once

## Visualising Gi\*

```{r}
tmap_mode("view")
tm_shape(HCSA) +
  tm_fill("gi_star") +
  tm_borders(alpha=0.5) + 
  tm_view(set.zoom.limits = c(6,8))
```

## Visualising p-value of HCSA

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("p_sim") +
  tm_borders(alpha=0.5)
```

## Visualising local HCSA

```{r}
tmap_mode("plot")
map1 <- tm_shape(HCSA) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "Gi* of GDPPC",
            main.title.size = 0.8)

map2 <- tm_shape(HCSA) +
  tm_fill("p_value",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of Gi*",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```


# Emerging Hot Spot Analysis

The data must be organised in the following form: 1 col year, 1 col location, 1 col for the value like Hunan_GDPPC.csv.

To organise the data, can look at Megan's report

## Installing and Loading the R packages

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse)
```

-   plotly to make the graph interactive "ggplotly(plot)"

## Import the geospatial data

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

## Import attribute table

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

## Creating a time series cube

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

-   spacetime takes in spatial data on the left, attribute info on the right gets combined & need to tell explicitly which field is location, which field is time

```{r}
GDPPC_st
```

-   has the aspatial df & geometry field

```{r}
GDPPC_nb <- GDPPC_st |>
  activate("geometry") |>
  mutate(
    nb = include_self(st_contiguity(geometry)),
    wt = st_weights(nb)
  ) |>
  set_nbs("nb") |>
  set_wts("wt")
```

-   

### Computing Gi\*

```{r}
gi_stars <- GDPPC_nb |>
  group_by(Year) |>
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt, nsim=99)) |>
      tidyr::unnest(gi_star)
gi_stars
```

## Mann Kendall Test

```{r}
cbg <- gi_stars |>
  ungroup() |>
  filter(County == "Changsha") |>
  select(County, Year, gi_star)
```

-   can choose any County

```{r}
#| eval: false
ggplot(data = cbg,
       aes(x = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()
```

## Performing Emerging Hotspot Analysis

```{r}
pacman::p_load(Kendall)
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st,
  .var = "GDPPC",
  k = 1,
  nsim = 99
)
```

```{r}
ggplot(data = ehsa,
       aes(x=classification)) + 
  geom_bar()
```

