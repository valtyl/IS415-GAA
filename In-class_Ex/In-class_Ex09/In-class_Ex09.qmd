---
title: "In-class Exercise 9: Geospatial Predictive"
execute: 
  message: true
  warning: false
  echo: true
  eval: true
date: "13 March 2023"
date-modified: "`r Sys.Date()`"
# number-sections: true
editor: visual
format: html
---

# 1 Overview

# 2 The Data

# 3 Getting Started

## 3.1 Install and load R packages

```{r}
pacman::p_load(sf, spdep, GWmodel, SpatialML, rsample, tidyverse, #this line of packages is enough for model calibration
               tmap, ggpubr, olsrr, devtools, tidymodels)
```

-   GWmodel: calibrate geographical models
-   SpatialML: calibrate spatial random forest tidyverse: manipulate dataframes
-   ggpubr: stitch multiple graphs together
-   olsrr: used for model diagnostic purposes eg computing VIF and checking multi-collinearity
-   devtools: import packages that are not in CRAN
-   tidymodels: advanced way to create ml modelling workflow

## 3.2 Preparing data

### 3.2.1 Reading data file to rds

Reading the data file. It is in simple feature data frame.

```{r}
mdata <- read_rds("data/aspatial/mdata.rds")
```

### 3.2.2 Data Sampling

The entire data are split into training and test data of 65% and 35% respectively.

```{r}
#| eval: false
set.seed(1234)
resale_split <- initial_split(mdata,
                              prop = 6.5/10)
train_data <- training(resale_split)
test_data <- testing(resale_split)
```

Write the result out so that we don't have to keep running the randomization step again

```{r}
#| eval: false
write_rds(train_data, "data/model/train_data.rds")
write_rds(test_data, "data/model/test_data.rds")
```

## 3.3 Computing Correlation Matrix

```{r}
mdata_nogeo <- mdata %>%
  st_drop_geometry()
corrplot::corrplot(cor(mdata_nogeo[, 2:17]),
                   diag = FALSE,
                   )
```

## 3.4 Retrieving the Stored Data

```{r}
train_data <- read_rds("data/model/train_data.rds")
test_data <- read_rds("data/model/test_data.rds")

```

## 3.5 Building a non-spatial multiple linear regression

```{r}
price_mlr <- lm(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                data = train_data)
summary(price_mlr)
```

For predictive model, we need to load the training data. Statistics obtained is not important for predictive model.

Residual standard error from above output \^ is not the same as OOB prediction error (MSE) in section 3.8 to make it the same, need to manually square root OOB prediction error (MSE)

we can compare Residual standard error from above output \^ and R squared (OOB) in section 3.8 to see which model does better. Section 3.8 has a bigger value hence random forest model does better. But this way of comparing is not as accurate as comparing with the manually square rooted value of OOB prediction error (MSE) from section 3.8

```{r}
#| eval: false
write_rds(price_mlr, "data/model/price_mlr.rds")
```

## 3.6 GWR predictive method

### 3.6.1 Converting the sf data.frame to SpatialPointDataFrame

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

### 3.6.2 Computing adaptive bandwidth

```{r}
#| eval: false
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                data = train_data_sp,
                approach = "CV",
                kernel="gaussian", 
                adaptive=FALSE, 
                longlat=FALSE)
```

### 3.6.3 Constructing the adaptive

```{r}

```

## 3.7 Preparing coordinates data

### 3.7.1 Extracting coordinates data

extract coordinates data from simple feature file and put into a vector table for ranger (ranger does not accept simple feature format so need to change train and test data)

```{r}
#| eval: false
coords <- st_coordinates(mdata)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

```{r}
#| eval: false
coords_train <- write_rds(coords_train, "data/model/coords_train.rds")
coords_test <- write_rds(coords_test, "data/model/coords_test.rds")
```

```{r}
coords_train <- read_rds("data/model/coords_train.rds")
coords_test <- read_rds("data/model/coords_test.rds")
```


### 3.7.2 Dropping geometry field

```{r}
train_data <- train_data %>%
  st_drop_geometry()
```

## 3.8 Calibrating Random Forest

```{r}
set.seed(1234)
rf <- ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                data = train_data)
```

This random forest is the base ranger. We can calibrate many different kinds of model

```{r}
print(rf)
```

## 3.9 Calibrating Geographically Weighted Random Forest Model

### 3.9.1 Calibrating using training data

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                dframe = train_data,
                bw = 55, #bandwidth
                kernel = "adaptive", #meaning 55 nearest transactions
                coords = coords_train)
```

-   train_data is a dataframe without coordinates, hence we need to tell what are the coordinates (which is coords_train)
-   if we use adaptive kernel, then bandwidth is the number of observations/neighbours (eg. closest 55 neighbours, closes 55 transaction points)
-   if we use fixed bandwidth, then bandwidth is the distance
-   look at grf() of [SpatialML](https://cran.r-project.org/web/packages/SpatialML/SpatialML.pdf) documentation
-   how to determine the bandwidth?
    -   borrow bandwidth from GWR method
    -   use grf.bw() to calculate the best bandwidth to work with (see in class ex 8)

The Report (output) has 2 parts:

1.  using as explanatory variable - Mean squared error (OOB) and 3 lines after
2.  using as predictive model - Mean squared error Predicted (OOB) and 3 lines after (AICc and AIC would be v close if there is no biasness in terms of the sampling)

Saving the model output:

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

inside gwRF_adaptive: 
`gwRF_adaptive$Global.Model`, `gwRF_adpative$Global.Model$variable.importance` (use this to find out which predictor contributes the most) (make it into a dataframe so we can use it: `vi_df <- as.data.frame(gwRF_adaptive$Global.Model$variable.importance)`)


```{r}
#| eval: false
vi_df <- as.data.frame(gwRF_adaptive$Global.Model$variable.importance)
vi_df
```
^ takes awhile to run

```{r}

```

### Predicting using test data

#### Preparing the test data
combine test data with its corresponding coordinates data.
coords_test contains x and y so will add 2 more variables.
st_drop_geometry will drop geometry column.
thus 18 variables + 2 - 1 = 19 variables
```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```


#### Predicting with test data
```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive,
                         test_data,
                         x.var.name = "X", # x coordinate
                         y.var.name = "Y", # y coordinate
                         local.w=1,
                         global.w=0)
```
^ takes awhile to run
coordinates must be projected coordinates system (SVY21), decimal degree coodinates will not work.
local.w = 1 means that we want it to calibrate the local version of the model
the output is a VECTOR

#### Converting the predicting output into a dataframe
convert the vector into a dataframe so that the dataframe can be combined with the original test data so that we can do further analysis like plotting predicted value and actual value to see how well they fit together
```{r}
#| eval: false
gwRF_pred_df <- as.data.frame(gwRF_pred)
```

```{r}
#| eval: false
gwRF_test_predict <- cbind(test_data, predict_grf_df)
```

```{r}
#| eval: false
write_rds(test_predict, "data/model/test_predict.rds")
```

```{r}
#| eval: false
ggplot(data = test_predict,
       aes(x = predict_grf,
           y = resale_price)) +
  geom_point()
```

```{r}
#| eval: false
sqrt(mean((test_predict$resale_price - test_predict$predict_grf)^2))
```

